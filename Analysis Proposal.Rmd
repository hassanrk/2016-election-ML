---
title: "Analysis Proposal [Group: High Expectasians]"
author: "Hassan Rahim Kamil (rahimka2), Weiliang Hu (whu17)"
date: "11/5/2018"
output:
  html_document:
    df_print: paged
---

# Sentiment Analysis of Trump's and Clinton's 2016 Tweets and Predicting Public Opinions

***

```{r setup-chunk, message = FALSE, warning = FALSE, echo = F}
pkg_list = c("tidyverse", "caret", "MASS", "tm", "tidytext", "syuzhet", "rtweet", "twitteR", "SnowballC", "devtools", "gtrendsR", "jsonlite", "rpart.plot", "pollstR", "RSentiment", "broom")
mia_pkgs = pkg_list[!(pkg_list %in% installed.packages()[,"Package"])]
if(length(mia_pkgs) > 0) install.packages(mia_pkgs)
loaded_pkgs = lapply(pkg_list, require, character.only=TRUE)
```

```{r reading data, cache = T, echo = F}
tweets = read.csv("tweets.csv") 
polls = read.csv("presidential_polls.csv")

hillary = read.csv("hillary-tweets.csv", header = T, sep = ">")
trump = fromJSON("trump-tweets.json")

```



## Introduction 

Microblogging is becoming more and more commonplace among today's generation. In fact, interactions with some type of online platforms or service such as Facebook, Twitter or Google leave traces of data that show a record of behavior or actions. As Davidowitz put it in his book *Everybody Lies*: "The everyday act of typing a word or phrase into a compact, rectangular white box leaves a small trace of truth that, when multiplied by millions, eventually reveals profound realities." To statisticians, this quote faintly reminds of the many statistical ideas that could be put to the test.

An emerging role of online platforms has been in the political context. As such, our research topic would be to predict political outcomes with a preferred dataset that is in line with our hypothesis and allows us to scientifically control for many variable.

> Hypothesis: We could use sentiments on social media to predict public opinions as a proxy for political outcomes.

The scope of the preferred dataset(s) that we are looking into include observations 

- whose geolocations match those of the political outcomes of interest.
- represent the voter base. And;
- are longitudinal. 

According to [Google Trends](https://trends.google.com/trends/story/election2016), the keywords most searched on Google during the 2016 election are "Abortion", "Immigration", "Race Issues", "Economy", "Affordable Care Act", "ISIS", "Climate Change", "National Debt", "Gun Control" and "Voting System." Using `gtrendsR`, we randomly picked five of those keywords and queried its search hits by setting `geo = "US"` to prevent bias.

```{r trend 1, cache = T, warning = F}
trend1 = gtrends(c("immigration", "abortion", "economy", "gun control", "terrorism"), geo = "US", time = "2016-01-01 2016-12-31")

plot(trend1)
```

Lo and behold, the trends do not differ much when we queried for Trump and Hillary.

```{r trend 2, cache = T}
trend2 = gtrends(c("hillary", "trump"), geo = "US", time = "2016-01-01 2016-12-31")

plot(trend2)
```

The trends are calculated on a weekly basis. How these trends are calculated can be viewed [here](https://support.google.com/trends/answer/4365533?hl=en). We see that these trends look more similar around mid-campaign.

Another popular measure of public opinions are by using polls. To that end, we queried [HuffPost Pollster](https://elections.huffingtonpost.com/pollster), a poll that aggregates every poll that claims to represent the population using `pollstR`.

```{r pollster, eval = F}
slug = "2016-general-election-trump-vs-clinton"
polls = pollster_charts_polls(slug) %>% .[["content"]]


```


We hypothesize that these trends are invoked by the use of sentiments from both political candidates during their campaigns either through speech or social media. Using the trends as the labels and the sentiments as the features, we could attempt to build a model to find out their interactions.

Some packages that we **may** use for this project:

```
tidyverse: 
A collection of R packages designed for data science. Useful for reading, wrangling and visualizing data.

caret:
A package to execute ML algorithms in R.

MASS:
A package that has functions and datasets to support Venables and Ripley, "Modern Applied Statistics with S" (4th edition, 2002).

tm: 
A package for text mining.

gtrendsR:
A package to query Google Trends.

pollstR:
A package to query Huffpost Pollster.

syuzhet, tidytext, RSentiment, SentimentAnalysis:
A collection of packages for sentiment analysis.

rtweet, TwitteR:
Packages to scrape from Twitter API.

```


***

## Method/ Approach/ Design

For demonstration of the methods that we plan to use: 

#### 1) Scraping Twitter API and Cleaning Data

As observed from the trends, we plan to extract tweets where search hits of Trump and Hillary match those of the top keywords on Google Trends. To that end, we will be scraping tweets from `@realDonaldTrump` and `@HillaryClinton`. This is easily done with either `twitteR` or `rtweet` by querying the info needed from the Twitter API. However, as of July 2018, a Twitter developer account needs to be made prior before we could provide a token to access the Twitter API. As of writing this paragraph, Hassan is waiting for his Twitter developer account to be approved.

For demonstration (and a backup dataset), we have obtained a dataset from [Kaggle](https://www.kaggle.com/benhamner/clinton-trump-tweets) of Trump's and Clinton's tweets from 01/01/2016 to 09/28/2016. Our then-scraped dataset will more or less look like the backup dataset. An overview of the dataset:


|         Features        |           Description           |
|-------------------------|---------------------------------|
| handle                  | Twitter handle name             |
| text                    | Tweets                          |
| is_retweet              | Whether the tweet was retweeted |
| original_author         | Original author                 |
| time                    | Timestamp                       |
| in_reply_to_screen_name | -                               |
| in_reply_to_status_id   | -                               |
| in_reply_to_user_id     | -                               |
| is_quote_status         | Whether the tweet was quoted    |
| lang                    | Twitter's guess at language     |
| retweet_count           | Retweet count                   |
| favorite_count          | Favorite count                  |
| longitude               | Longitude                       |
| latitude                | Latitude                        |
| place_id                | Place id                        |
| place_full_name         | Place full name                 |
| place_name              | Place name                      |
| place_type              | Place type                      |
| place_country_code      | Country code                    |
| place_country           | Country                         |
| place_contained_within  | Place contained within          |
| place_attributes        | Place attributes                |
| place_bounding_box      | Place bounding box              |
| source_url              | Tweet source url                |
| truncated               | Whether it is truncated         |
| entities                | a JSON object                   |
| extended_entities       | Another JSON object             |

The dimensions of the dataset:

```{r data, echo = F}
dim(tweets)
```

First five observations of both candidates:

`@HillaryClinton`:

```{r hillary, echo = F}

tweets %>% .[tweets$handle == "HillaryClinton", ] %>% head(5)

```

`@realDonaldTrump`:

```{r trump, echo = F}

tweets %>% .[tweets$handle == "realDonaldTrump", ] %>% head(5)

```

In sentiment analysis, we would treat the texts as a corpus object and clean them by using the function `tm_map()` from the `tm` package until we are only left with the sentiments.

```{r creating corpus, echo = F}

corpus = as.character(tweets[, "text"]) %>% VectorSource() %>% Corpus()

corpus
```

```{r cleaning, results = "hide", warning= F}

cleaned_corpus = tm_map(corpus, tolower) %>%
  tm_map(PlainTextDocument) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, stopwords("english")) %>%
  tm_map(stripWhitespace)

cleaned_corpus

```

Alternatively, another approach is to use the `tidytext` or `quanteda` package to clean the corpus object.

For this, since the labels are defined weekly, we have to make sure that the strings are concatenated weekly before only the sentiments are kept. To that end, we could achieve this by using the functional `tapply()` and the function `string_c` (from the `stringr` package) after we have processed the timestamps to become a `week` factor so that the variables match the labels.


#### 2) Analyzing the Textual Data

After obtaining only the sentiments, we would transform them into a Document-Term Matrix, which calculates the frequencies at which the sentiments appear at each observation.

For example, we have the following two texts:

```
[1]
"I like STAT432"

[2]
"I hate STAT432"
```

A document-term matrix would show:

| id | I | hate | like | STAT432 |
|----|---|------|------|---------|
|  1 | 1 |   0  |   1  |    1    |
|  2 | 1 |   1  |   0  |    1    |

It is important that we only obtain the sentiments so that we could treat the frequencies as our predictors before performing our analysis.

Furthermore, we would define a popularity vector $\mathbf{P}$ as

$$\mathbf{P} = \frac{|\{w_i : w_i \in W\} |_t}{|W|_t} \in \{[0, 1]\}_{t=1}^n$$
, which is approximately how the Google Trend's search hits are defined after adjusting for time.

We would want to estimate $\mathbf{\hat{P}}$ using the sentiments as the predictors. We could perform a few analyses with this setup:

(a) **Logistic Regression**





